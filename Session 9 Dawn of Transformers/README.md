# Session 9 Assignment 

To Build a Basic Tranformer Network Architecture and Train it for 24 Epochs on CIFAR10 Dataset Using One Cycle Policy 

# Network Architecture

![image](https://user-images.githubusercontent.com/61132761/222738400-893ce5a8-3abb-4666-aa6a-80d508af8493.png)

# Images Misclassified by the Model 

![image](https://user-images.githubusercontent.com/61132761/222738738-84e1ca1d-fad1-4395-b221-73dd77ff3ab3.png)

# Relevance of Transformer Network in Modern World 

The Transformer network has become an indispensable tool in the modern world, revolutionizing natural language processing, computer vision, and other fields of machine 
learning. Its ability to effectively model sequential data through self-attention mechanisms has made it a popular choice for a wide range of applications, including 
language translation, sentiment analysis, and image classification. The Transformer has also facilitated the development of new and more complex deep learning 
architectures, enabling researchers and practitioners to tackle increasingly challenging problems. As the demand for sophisticated machine learning models continues to 
grow, the Transformer network's relevance will only continue to increase in the modern world.

Below is the Transformer Network with encoder and Decoder Block 

![image](https://user-images.githubusercontent.com/61132761/222740823-2ca841ee-199f-4dee-bcdd-0c90363bc8c7.png)

# External Reference

https://github.com/jacobgil/pytorch-grad-cam
