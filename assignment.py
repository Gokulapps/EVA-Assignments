# -*- coding: utf-8 -*-
"""Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18TyY1pMUg8K23ZQpEwaXT-Eo4ziGzyn3
"""

# Importing all Necessary Libraries in a Single Cell to make it Easier
import torch 
import torchvision 
import torchvision.transforms as transforms 
import torch.nn as nn 
import torch.nn.functional as F 
import torch.optim as optim 
import torch.autograd as grad
import matplotlib.pyplot as plt 
import numpy as np 
import pandas as pd
from torch.utils.data import Dataset

# Downloading Training Dataset and Loading them into as Batch using DataLoader
train_set = torchvision.datasets.FashionMNIST(
    root = './data', # Training Data Location
    train = True, # Train Dataset
    download = True, # Downloading
    transform = transforms.Compose([
        transforms.ToTensor() # Converting it into an Tensor 
        ])
)
batch_size = 100 # Batch Size
train_loader = torch.utils.data.DataLoader(
    train_set, # Loading train_set Dataset as a Batch
    batch_size = batch_size, # Defining Batch Size
    shuffle = True # Shuffling
)

# Plotting a Batch of Images from the Dataset in order to make Inference 
batch = next(iter(train_loader)) # getting an Batch of 100 Images from Total Dataset of 60000 Images
images, labels = batch # Unpacking Images and Labels in Different Variables
grid = torchvision.utils.make_grid(images, nrow = 15) # Creating Grid 
plt.figure(figsize = (20, 20)) # Defining Output Figure Size
plt.imshow(np.transpose(grid, (1, 2, 0))) # Displaying

# Building a Neural Network 1 which takes two inputs(image and random number) then mixes the Convoluted Output of an Image and Random Number at Fully Connected Layer to produces two Outputs.
class Network1(nn.Module):
  def __init__(self, output_class, batch_size):
    super().__init__()
    self.output_class = output_class 
    self.batch_size = batch_size
    self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = 3)# input = 28*28, output = 26*26, RF = 3, kernel_size = 3*3
    self.conv2 = nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = 3)# input = 26*26, output = 24*24, RF = 5, kernel_size = 3*3
    self.conv3 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3)# input = 24*24, output = 22*22, RF = 7, kernel_size = 3*3
    self.conv4 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3)# input = 22*22, output = 20*20, RF = 9, kernel_size = 3*3
    self.conv5 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3)# input = 20*20, output = 18*18, RF = 11, kernel_size = 3*3
    # Max Pooling Layer - input 18*18, kernel_size = 2*2, stride = 2, output = 9*9
    self.fc1 = nn.Linear(in_features = 128*9*9+10, out_features = 120) # input = 128*9*9+10(No of channels-128, Image Size-9*9, Random Number-10), output = 120
    self.fc2 = nn.Linear(in_features = 120, out_features = 60) # input = 120, output = 60
    self.output = nn.Linear(in_features = 60, out_features = self.output_class) # input = 60, output = 29
  def forward(self, tensor, random_num):
    # Input Layer
    x = tensor
    # Convolution Layer 1
    x = self.conv1(x)
    x = F.relu(x)
    # Convolution Layer 2 
    x = self.conv2(x)
    x = F.relu(x)
    # Convolution Layer 3
    x = self.conv3(x)
    x = F.relu(x)
    # Convolution Layer 4 
    x = self.conv4(x)
    x = F.relu(x)
    # Convolution Layer 5 
    x = self.conv5(x)
    x = F.relu(x)
    # Max Pooling 
    x = F.max_pool2d(x, kernel_size = 2, stride = 2)
    # Flattening Our Tensor and adding the one hot encoded value of the Random Number. 
    x = x.reshape(self.batch_size, -1) # Flattening of Tensor
    x = torch.cat((x, F.one_hot(random_num, num_classes=10)), dim = 1) # num_classes = 19 (0 - 18) Possible Values.
    # Fully Connected Layer 1 
    x = self.fc1(x)
    x = F.relu(x)
    # Fully Connected Layer 2
    x = self.fc2(x)
    x = F.relu(x)
    # Output Layer
    x = self.output(x)
    x = F.softmax(x, dim = 1)

    return x[:,:10], x[:,10:]
# Number of Output class is equal to sum of unique classes of the dataset and the 19 possible values for the second output
output_class = torch.unique(train_set.targets).shape[0] + 19 # 19 indicates the possible values for sum of prediction and the random number[0 - 18]
network1 = Network1(output_class, batch_size)

print(network1) # Verifying the Network

# Function to get Number of Correct Predictions from two outputs and 2 labels
def get_correct_predictions(prediction1, prediction2, labels1, labels2): 
  pred1_correct = prediction1.argmax(dim = 1).eq(labels1) # Compares Prediction1 with labels1 and return true or false
  pred2_correct = prediction2.argmax(dim = 1).eq(labels2) # Compares Prediction2 with labels2 and return true or false
  # Now we need to multiply prediction1 with prediction2 to get Correct Predictions in both the cases and then sum them up. 
  pred_correct = (pred1_correct * pred2_correct).sum().item() 
  return pred_correct # Returning the Total Number of Correct Predictions

# Function to Train the Network with all the Images in the Dataset and Running it for 10 Epochs to Improve the Accuracy.
def model_training1(dataset, epoch_no, network): 
  optimizer = optim.Adam(params = network.parameters(), lr = 0.001) # Defining Optimizer for the Neural Network (Adam in this case)
  for epoch in range(1, epoch_no + 1): # Outer for loop to train the Network with Multiple Epochs
    total_loss = 0
    total_correct = 0 
    for batch_no, batch in enumerate(dataset, start = 1): # Inner for loop to train the network with batch of Images
      images, labels1 = batch # Unpacking Images and Labels
      random_value = torch.randint(0, 10, (batch_size, 1)).squeeze() # Generating Random Numbers in range 0 - 9 as 1D Tensor(Batch Size)
      labels2 = random_value + labels1 # Defining Labels2 for the Output2 which is sum of Random Number and Label1
      prediction1, prediction2 = network(images, random_value) # Passing the batch of Images, Random Values to the Network 
      optimizer.zero_grad() # Making the Gradients Zero to Release the Gradients of Previous batch
      loss = F.cross_entropy(prediction1, labels1) + F.cross_entropy(prediction2, labels2) # Summing the Loss for both the Outputs.
      total_loss += loss.item() # Adding it to the Total Loss for the Current Epoch
      total_correct += get_correct_predictions(prediction1, prediction2, labels1, labels2) # Total Correct
      loss.backward() # Calculating the Gradients
      optimizer.step() # Updating the Weights of the Network
    print(f'Epoch {epoch} Result :-')
    print(f'Total Images Processed in this Epoch : {batch_no * batch_size}')
    print(f'Loss for this Current Epoch : {total_loss}')
    print(f'Total Number of Correct Predictions in this Current Epoch : {total_correct}')
  print(f'Model Training Completed!!!!')

model_training1(train_loader, 10, network1) # Calling the model_training1 Function

# Building a Neural Network 2 which takes one input and produces two outputs 
class Network2(nn.Module):
  def __init__(self, output_class, batch_size):
    super().__init__()
    self.output_class = output_class 
    self.batch_size = batch_size
    self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = 3)# input = 28*28, output = 26*26, RF = 3, kernel_size = 3*3
    self.conv2 = nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = 3)# input = 26*26, output = 24*24, RF = 5, kernel_size = 3*3
    self.conv3 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3)# input = 24*24, output = 22*22, RF = 7, kernel_size = 3*3
    self.conv4 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3)# input = 22*22, output = 20*20, RF = 9, kernel_size = 3*3
    self.conv5 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3)# input = 20*20, output = 18*18, RF = 11, kernel_size = 3*3
    # Max Pooling - input 18*18, kernel_size = 2*2, stride = 2, output = 9*9
    self.fc1 = nn.Linear(in_features = 128*9*9, out_features = 120)
    self.fc2 = nn.Linear(in_features = 120, out_features = 60)
    self.output = nn.Linear(in_features = 60, out_features = self.output_class)
  def forward(self, tensor):
    # Input Layer
    x = tensor
    # Convolution Layer 1
    x = self.conv1(x)
    x = F.relu(x)
    # Convolution Layer 2 
    x = self.conv2(x)
    x = F.relu(x)
    # Convolution Layer 3
    x = self.conv3(x)
    x = F.relu(x)
    # Convolution Layer 4 
    x = self.conv4(x)
    x = F.relu(x)
    # Convolution Layer 5 
    x = self.conv5(x)
    x = F.relu(x)
    # Max Pooling 
    x = F.max_pool2d(x, kernel_size = 2, stride = 2)
    # Flattening Our Tensor
    x = x.reshape(self.batch_size, -1) # Flattening of Tensor
    # Fully Connected Layer 1 
    x = self.fc1(x)
    x = F.relu(x)
    # Fully Connected Layer 2
    x = self.fc2(x)
    x = F.relu(x)
    # Output Layer
    x = self.output(x)
    x = F.softmax(x, dim = 1)

    return x[:,:10], x[:,10:]
# Number of Output class is equal to sum of unique classes of the dataset and the 19 possible values for the second output
output_class = torch.unique(train_set.targets).shape[0] + 19 # 19 indicates the possible values for sum of prediction and the random number[0 - 18]
network2 = Network2(output_class, batch_size)

print(network2) # Verifying the Network

# Function to Train the Network with all the Images in the Dataset and Running it for 10 Epochs to Improve the Accuracy.
# In this Function Each Image in a Batch is Modified to include Random Numbers. Modified Image is Passed as Input to the Network.
def model_training2(dataset, epoch_no, network): 
  optimizer = optim.Adam(params = network.parameters(), lr = 0.001) # Defining Optimizer for the Neural Network (Adam in this case)
  for epoch in range(1, epoch_no + 1): # Outer for loop to train the Network with Multiple Epochs
    total_loss = 0 
    total_correct = 0 
    for batch_no, batch in enumerate(dataset, start = 1): # Inner for loop to train the network with batch of Images
      images, labels1 = batch # Unpacking Images and Labels
      temp_labels = torch.randint(0, 10, (batch_size, 1)).squeeze() # Generating Random Numbers in range 0 - 9 as 1D Tensor(Batch Size)
      random_value = F.one_hot(temp_labels, num_classes = 10) # Converting Random Numbers into One Hot Encoded Values of 10 classes  
      for index in range(batch_size): # for loop to iterate through each image in the Batch
        images[index, :, -1, -10:] = random_value[index] # Replacing last ten values of 28*28 Image with this One Hot Encoded Random Values
      labels2 = temp_labels + labels1 # Summing labels1 and random numbers to Get Labels2 for the Second Output.
      prediction1, prediction2 = network(images) # Passing the batch of Images, Random Values to the Network 
      optimizer.zero_grad() # Making the Gradients Zero to Release the Gradients of Previous batch
      loss = F.cross_entropy(prediction1, labels1) + F.cross_entropy(prediction2, labels2) # Summing the Loss for both the Outputs
      loss.backward() # Calculating the Gradients 
      optimizer.step() # Updating the weights of the Network
      total_loss += loss.item() # Adding it to the Total Loss for the Current Epoch
      total_correct += get_correct_predictions(prediction1, prediction2, labels1, labels2) # Total Correct
    print(f'Epoch {epoch} Result :-')
    print(f'Total Images Processed in this Epoch : {batch_no * batch_size}')
    print(f'Loss for this Current Epoch : {total_loss}')
    print(f'Total Number of Correct Predictions in this Current Epoch : {total_correct}')
  print(f'Model Training Completed!!!!')

model_training2(train_loader, 10, network2) # Calling the model_training2 Function